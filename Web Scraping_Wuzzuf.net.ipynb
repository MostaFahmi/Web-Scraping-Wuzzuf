{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945d0259",
   "metadata": {},
   "source": [
    "# | WEB SCRAPING: Wuzzuf.net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e81035",
   "metadata": {},
   "source": [
    "## 1 - Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Installation\n",
    "\n",
    "#!pip install lxml\n",
    "#!pip install requests\n",
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c576f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "#Optional Libraries\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import smtplib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c5eab",
   "metadata": {},
   "source": [
    "## 2- Create Data Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdc0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = []\n",
    "company = []\n",
    "address = []\n",
    "job_type = []\n",
    "job_skill = []\n",
    "link = []\n",
    "salary = []\n",
    "responsibility = []\n",
    "date = []\n",
    "description = []\n",
    "link = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae8b6a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Page Switched\n",
      "Pages Ended\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------FETCH THE DATA----------------------------------------------------\n",
    "\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "        page = requests.get(f\"https://wuzzuf.net/search/jobs/?q=data+analyst&a=hpb={page_num}\", headers=headers)\n",
    "\n",
    "        # Assign Page Source\n",
    "        src = page.content\n",
    "        #print(src)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------------------------------PARSE THE DATA--------------------------------------------\n",
    "\n",
    "        # Create Soup Object for Parsing\n",
    "\n",
    "        ugly_soup = BeautifulSoup(src, \"lxml\")\n",
    "        soup = BeautifulSoup(ugly_soup.prettify(), \"lxml\")\n",
    "        soup_skills = BeautifulSoup(ugly_soup.prettify(), \"lxml\")\n",
    "        #print(soup)\n",
    "\n",
    "\n",
    "\n",
    "        #pages_available = No.Of Results / Results per Page\n",
    "        No_Of_Results = int(soup.find(\"strong\").text)\n",
    "\n",
    "\n",
    "        if(page_num > No_Of_Results // 15):\n",
    "            print(\"Pages Ended\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #------------------------Getting The Underlined Elements----------------------\n",
    "\n",
    "\n",
    "        job_titles = soup.find_all(\"h2\",{\"class\":\"css-m604qf\"})\n",
    "        #print(len(job_titles))\n",
    "        #print(job_titles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        companies = soup.find_all(\"a\",{\"class\":\"css-17s97q8\"})\n",
    "        #print(len(companies))\n",
    "        #print(companies)\n",
    "\n",
    "\n",
    "        addresses = soup.find_all(\"span\",{\"class\":\"css-5wys0k\"})\n",
    "        #print(len(addresses))\n",
    "        #print(addresses)\n",
    "\n",
    "\n",
    "        job_types = soup.find_all(\"div\",{\"class\":\"css-1lh32fc\"})\n",
    "        #print(len(job_types))\n",
    "        #print(job_types)\n",
    "\n",
    "\n",
    "\n",
    "        # This line is looking for <span> elements that have either the class \"css-4c4ojb\" or \"css-do6t5g\",\n",
    "        # Because the posted/date info has two different classes\n",
    "        # If an element has either of these classes, it will be included in the date_elements list.\n",
    "\n",
    "        dates = soup.find_all(\"div\",class_=[\"css-4c4ojb\",\"css-do6t5g\"])\n",
    "        #print(len(dates))\n",
    "        #print(dates)\n",
    "\n",
    "\n",
    "\n",
    "        #job_skills: Because job skill has un-classed tag just <div>,\n",
    "        # I got in the parent tag but excluded the other child uneeded tag to inlcude only job skill un-classed tag.\n",
    "\n",
    "        job_skills = []\n",
    "        for div in soup_skills.find_all('div', {\"class\":'css-y4udm8'}):\n",
    "            job_type_span = div.find('span', {\"class\":'css-1ve4b75 eoyjyou0'})\n",
    "            if job_type_span:\n",
    "                job_type_span.extract()\n",
    "            job_skills.append(div)\n",
    "\n",
    "        #print(len(job_skills))\n",
    "        #print(job_skills)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #------------------------Getting The Underlined Information----------------------\n",
    "\n",
    "\n",
    "        # any list from the above will work not just len(jobskills) cuz they all equal to 15 indexes\n",
    "        for i in range(len(job_skills)):\n",
    "            job_title.append(job_titles[i].text.strip())\n",
    "\n",
    "\n",
    "            #link element <a> is already a branch in job_title element (<h2> \"class\":\"css-m604qf\")\\[i]: \n",
    "            link.append(job_titles[i].a.attrs[\"href\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            company.append(companies[i].text.strip())\n",
    "\n",
    "            cleaned_address = ' '.join(addresses[i].text.replace('\\n', '').split())\n",
    "            address.append(cleaned_address)\n",
    "\n",
    "            cleaned_job_type = ' '.join(job_types[i].text.replace('\\n', '').split())\n",
    "            job_type.append(cleaned_job_type)\n",
    "\n",
    "            cleaned_date = ' '.join(dates[i].text.replace('\\n', '').split())\n",
    "            date.append(cleaned_date)\n",
    "\n",
    "            cleaned_job_skill = ' '.join(job_skills[i].text.replace('\\n', '').replace('Â·',\"|\").split())\n",
    "            job_skill.append(cleaned_job_skill)\n",
    "\n",
    "        #print(job_title,company,address,job_type,date,job_skill,link)\n",
    "\n",
    "        page_num+=1\n",
    "        print(\"Page Switched\")\n",
    "    except:\n",
    "        print(\"Error Found\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------Internal Information from  The Link Container Created-------------------------------------\n",
    "\n",
    "# for internal description information\n",
    "\n",
    "for link_i in link:\n",
    "    #Page Fetch\n",
    "    job_page = requests.get(link_i,headers=headers)\n",
    "    #Page Source\n",
    "    job_src = job_page.content\n",
    "    #Parsing\n",
    "    job_soup = BeautifulSoup(job_src, \"lxml\")\n",
    "    \n",
    "    description_element = job_soup.find(\"div\",{\"class\":\"css-1uobp1k\"})\n",
    "    \n",
    "    # Doing the following to customize the Bullet Points ** in the job description\n",
    "    description_text = \"\"\n",
    "    # Here the class \"class\":\"css-1uobp1k\" had the Bullet Points in page with <p> and other pages with <li>\n",
    "    for p in description_element.find_all([\"p\",\"li\"]):\n",
    "        description_text += \"** \"+p.text+\"| \"\n",
    "    description_text = description_text[:-2]\n",
    "    description.append(description_text)\n",
    "        \n",
    "#print(description)\n",
    "\n",
    "\n",
    "\n",
    "# for internal Salary information\n",
    "\n",
    "#-------- Selenium for salary cuz JavaScript-----\n",
    "\n",
    "# Set the path to the ChromeDriver executable\n",
    "driver_path = '/path/to/chromedriver'\n",
    "\n",
    "# Set up Chrome options (optional)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
    "\n",
    "# Create a new instance of the Chrome driver with options\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Define the user agent (optional)\n",
    "\n",
    "# Iterate over the first three links\n",
    "for link_i in link:\n",
    "    # Navigate to the URL\n",
    "    driver.get(link_i)\n",
    "\n",
    "    # Wait for Java\n",
    "    #Script to load (you may need to adjust the wait time based on the website)\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Use Selenium to find elements\n",
    "    salary_elements = driver.find_elements(By.XPATH, '//span[@class=\"css-4xky9y\"]')\n",
    "\n",
    "    # Extract information in the 4th index because the class=\"css-4xky9y\" is not assoctiated with just SALARY.. \n",
    "    # but with Experience, Career level... , and the salary locate as the 4th index, this why I did that\n",
    "    if len(salary_elements) >= 4:\n",
    "        salary.append(salary_elements[3].text)\n",
    "    else:\n",
    "        print(\"Not enough elements in the list.\")\n",
    "#print(salary)\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc2bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------SAVE THE DATA---------------------------------------------------\n",
    "\n",
    "## Create CSV File & Fill It\n",
    "\n",
    "file_list = [job_title,company,description,salary,date,address,job_type,job_skill,link]\n",
    "extended_file_list = zip_longest(*file_list)\n",
    "\n",
    "with open(\"E:/The Journey/Career/Data Science/_Portfolio Projects/Python Projects/Web Scraping Projects/Web Scraping - bs4 Template/bs4Template.csv\", \"w\",encoding='utf-8', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"job_title\",\"company\",\"description\",\"salary\",\"date\",\"address\",\"job_type\",\"job_skill\",\"link\"])\n",
    "    wr.writerows(extended_file_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
